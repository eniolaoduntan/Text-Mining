# -*- coding: utf-8 -*-
"""Eniola Text Mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19W-c0GHyKSiUpQ9SkRnGo4J_rP3SsVAS
"""

import numpy as np
import os
import pandas as pd
import seaborn as sns
import re, string
from sklearn.model_selection import train_test_split
from matplotlib import pyplot
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import plotly.express as px
!pip install nltk
import nltk
nltk.download([
   "names",
   "stopwords",
   "state_union",
   "wordnet",
   "averaged_perceptron_tagger",
   "vader_lexicon",
   "punkt",
])
nltk.download('state_union')
nltk.download('stopwords')
nltk.download('punkt')
import io
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from wordcloud import WordCloud
from nltk.sentiment import SentimentIntensityAnalyzer

#import dataset
data = pd.read_csv('/content/Elon musk.csv')

"""# **INITIAL EXPLORATION**"""

data.shape

data.info()

data.head()

data.columns

data.isna().any()

"""# **TASK 1 DATA PREPROCESSING**

## **CREATING ADDITIONAL FEATURES**
"""

#create features to show whether tweets contain url or images
from urllib.parse import urlparse

def count_urls(row):
  text= row['Tweets']
  parsed = urlparse(text)
  if parsed.scheme and parsed.netloc:
    return 1
  else:
    return 0

data["num_urls"] = data.apply(count_urls, axis= 1)

def contains_image(row):
  text = row['Tweets']
  if 'pic.twitter.com' in text:
    return True
  else:
    return False

data["contains_image"] = data.apply(contains_image, axis= 1)

"""## **CHANGING DATA TYPES**"""

data['Tweets'] = data['Tweets'].astype(str)
data['Date'] = pd.to_datetime(data['Date'], infer_datetime_format=True)

data.info()

"""## **CLEANING TWEETS**"""

from google.colab import drive
drive.mount('/content/drive')

#Adding stop words
from nltk.corpus import stopwords
stop = stopwords.words("english")

#extend stopwords list
stop.extend(["from", "re","https", "co", "t", "I", "That","This","There","amp", 'It'])

print(stop)

data["Tweets"] = data["Tweets"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

#lowercase tweets
data['Tweets'] = data['Tweets'].apply(lambda x: " ".join(x.lower() for x in x.split()))

data['Tweets'] = data['Tweets'].astype(str)

#Handling Twitter Handles
def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)

    return input_txt

    #remove twitter handles
data['Tweets'] = np.vectorize(remove_pattern)(data['Tweets'], "@[\w]*")

#remove urls so as to not affect sentiment analysis
def cleaning_URLs(data):
    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)

def clean_data(data):
#replace URL of a text
    data['Tweets'] = data['Tweets'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')

data.head()

data['Tweets'] = data['Tweets'].astype(str)

#Tokenization
from nltk.tokenize import RegexpTokenizer

regexp = RegexpTokenizer('\w+')

data['Tweets_Token']=data['Tweets'].apply(regexp.tokenize)

data.head(20)

"""# **TASK 2 STATISTICAL ANALYSIS**"""

data.shape

data.describe()

data.columns

data.head

data.info()

data.isna().any()

data['Tweets'] = data['Tweets'].astype(str)

from nltk.probability import FreqDist
fd = FreqDist(data['Tweets'])
print(fd)
fd.most_common(5)

from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder
words = data['Tweets'].apply(nltk.word_tokenize).sum()
# Create a Text object
text_3 = nltk.Text(words)
# Find bigram collocations
finder = BigramCollocationFinder.from_words(text_3, window_size=3)
scored = finder.score_ngrams(BigramAssocMeasures().raw_freq)
collocations = sorted(scored, key=lambda x: -x[1])
print(collocations[:10])

"""# **DATA VISUALISATION**"""

#Visualising the Most Common Words
# Obtain top 10 words
top_10 = fd.most_common(10)

# Create pandas series to make plotting easier
fdist = pd.Series(dict(top_10))

#plot of common words
sns.set_theme(style="ticks")

sns.barplot(y=fdist.index, x=fdist.values, color='blue');

data1 = data[data['Date'].dt.year.isin([2022])]

#Convert to a pandas DatetimeIndex
year = pd.DatetimeIndex(data1['Date']).year
hour = pd.DatetimeIndex(data1['Date']).hour
month = pd.DatetimeIndex(data1['Date']).month


#Create bins for diffrent times of the day
bins = [0, 11, 16, 23]
labels = ['Morning', 'Afternoon', 'Evening']
time_of_day = pd.cut(hour, bins=bins, labels=labels)

#Group the tweets by month and time of day
grouped = data1.groupby([month, time_of_day])['Tweets'].count().unstack()

# Visualise using stacked bars
ax = grouped.plot(kind='bar', stacked=True, title='Tweet Frequency by Time')
ax.set_xlabel('Month')
ax.set_ylabel('Number of Tweets')
ax.legend(title='Time')
plt.show()

#create a new feature named Month
data["Month"] = pd.DatetimeIndex(data1['Date']).month

data.Month.value_counts().plot.barh()

sns.catplot(data= data, x="Month", y="Likes", kind="bar")

figure = px.histogram(data, x = "Month",
                      y = "Retweets",
                      title= "Retweets by Month")
figure.show()

#Create new feature Hour
data['Hour'] = pd.DatetimeIndex(data1['Date']).hour

sns.catplot(data= data, x="Hour", y="Likes", kind="bar")

#create new feature Time
data["Time"] = time_of_day

data.Time.value_counts().plot.barh()

sns.catplot(data= data, x="Time", y="Retweets", kind="bar")

sns.catplot(data= data, x="Time", y="Likes", kind="bar")

#create new feature tweet length
data['TweetLength'] = data['Tweets'].apply(len)

"""# **TASK 3 EXPLORATORY DATA MINING**

### **WORD CLOUD**
"""

#Generate a Word Cloud
wordcloud = WordCloud(width=600,
                     height=400,
                     random_state=2,
                     max_font_size=100).generate(' '.join(data["Tweets"]))

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off');

"""### **SENTIMENT ANALYSIS**"""

from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

analyzer= SentimentIntensityAnalyzer()

(data['TweetScore'])= data['Tweets'].apply(analyzer.polarity_scores)
data.head()

data.tail()

def sentiment(row):
  text = row['TweetScore']
  polarity = "neutral"

  if(text['compound']>= 0.05):
    polarity = "positive"

  elif(text['compound'] <= -0.05):
    polarity = "negative"

  return polarity

data["Sentiment"] = data.apply(sentiment, axis= 1)

data.head()

data["Sentiment"].value_counts()

data.Sentiment.value_counts().plot.barh()

toplikes= data[(data['Likes'] >= 2500000)]
toplikes.head(10)

data[(data['Sentiment'] == 'negative')]

#create a new column based on compound scores
data['Compound'] = [analyzer.polarity_scores(x)['compound'] for x in data['Tweets']]

data.head()

data.info()

#Sort by number of likes and choose top 5 tweets
top5_tweets = data.sort_values("Likes", ascending=False).head(5)

for i, tweet in top5_tweets.iterrows():
    print(f"{tweet['Tweets']}\nLikes: {tweet['Likes']}\n")

# Create dataFrame showing counts of positive, negative, and neutral tweets
sentiment_c = top5_tweets.groupby(['Sentiment'])['Tweets'].count().reset_index()

sentiment_c

# Create a stacked bar
sns.set_style('whitegrid')
plt.bar(sentiment_c['Sentiment'], sentiment_c['Tweets'], color=['blue', 'red', 'purple'])
plt.title('Sentiment behind Top 5 Most Liked Tweets')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

"""# **TASK 4 MACHINE LEARNING**"""

#Prepare the data to be fed into the modelling tool
data['Sentiment'] = data['Sentiment'].replace("neutral", 0)
data['Sentiment'] = data['Sentiment'].replace("negative", 1)
data['Sentiment'] = data['Sentiment'].replace("positive", 2)

#Handle Missing Values in Time
data['Time'].fillna('Morning', inplace = True)

data['Time'] = data['Time'].replace("Morning", 0)
data['Time'] = data['Time'].replace("Afternoon", 1)
data['Time'] = data['Time'].replace("Evening", 2)

data['Time'] = pd.to_numeric(data['Time']).astype('Int64')

data.info()

import scipy.stats as stats
corr, _ = stats.pearsonr (data['Compound'], data['Likes'])
corr

"""### **Building the Models**"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.tree import DecisionTreeRegressor

y = data.Likes
x = pd.DataFrame(data[['TweetLength', 'Hour', 'Month', 'Retweets', 'Compound']])
# implementing train-test-split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state = 23)

#LINEAR REGRESSION
lm = LinearRegression()
model = lm.fit(x_train, y_train)

print(model.intercept_)

print(model.coef_)

pd.DataFrame(model.coef_, x.columns, columns = ['Coeff'])

"""from the coefficient, it is seen that as the the sentiment increases by one unit (in this case from neutral to negative to positive, the number of likes decreases"""

#get the r squared
model.score(x_train, y_train)

#make predictions
lm_pred = model.predict(x_test)

plt.scatter(y_test, lm_pred)

#Testing Performance
import sklearn.metrics as sm
print("Mean absolute error =", round(sm.mean_absolute_error(y_test, lm_pred), 2))

#root mean squared error
np.sqrt(sm.mean_squared_error(y_test,lm_pred))

score =r2_score(y_test, lm_pred)
print("The accuracy of our model is {}%".format(round(score,2)*100))

#RANDOM FOREST REGRESSOR
from sklearn.ensemble import RandomForestRegressor

# Create the Random Forest Regression model
rf = RandomForestRegressor(n_estimators=100, random_state=23)

# Fit the model on the training data
rf.fit(x_train, y_train)

rf.score(x_train, y_train)

# Make predictions on the testing data
rf_pred = rf.predict(x_test)

# Calculate the mean squared error
print("Mean absolute error =", round(sm.mean_absolute_error(y_test, rf_pred), 2))

np.sqrt(sm.mean_squared_error(y_test,rf_pred))

model2score =r2_score(y_test, rf_pred)
print("The accuracy of our model is {}%".format(round(model2score,2)*100))

#DECISION TREE REGRESSOR
regressor = DecisionTreeRegressor()
model3 = regressor.fit(x_train, y_train)

regressor_pred =regressor.predict(x_test)

print("Mean absolute error =", round(sm.mean_absolute_error(y_test, regressor_pred), 2))

np.sqrt(sm.mean_squared_error(y_test,regressor_pred))

model3score =r2_score(y_test, regressor_pred)
print("The accuracy of our model is {}%".format(round(model3score,2)*100))